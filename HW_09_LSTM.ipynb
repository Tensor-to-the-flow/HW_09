{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW09editedincomplete.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCmmWreYlGAt"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fl7JKAttWYn"
      },
      "source": [
        "# Simple one-cell Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af272FEPpBRe"
      },
      "source": [
        "class LSTM_Cell(tf.keras.layers.Layer):\r\n",
        "  def __init__(self):\r\n",
        "    super(LSTM_Cell,self).__init__()\r\n",
        "    self.fgate = tf.keras.layers.Dense(H_size,activation='sigmoid',bias_initializer=tf.keras.initializers.Ones())\r\n",
        "    self.ingate = tf.keras.layers.Dense(H_size,activation='sigmoid')\r\n",
        "    self.cand = tf.keras.layers.Dense(H_size,activation='tanh')\r\n",
        "    self.outgate = tf.keras.layers.Dense(H_size,activation=\"sigmoid\")\r\n",
        "    \r\n",
        "  def call(self,input,hidden_state,cell_state):\r\n",
        "    concat_input = tf.concat((hidden_state,input),axis=1)\r\n",
        "    cell_state = cell_state * self.fgate(concat_input)\r\n",
        "    update = self.ingate(concat_input) * self.cand(concat_input)\r\n",
        "    cell_state = cell_state + update\r\n",
        "    output_hidden_state = tf.keras.activations.tanh(cell_state)*self.outgate(concat_input)\r\n",
        "    return output_hidden_state,cell_state\r\n"
      ],
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZR-z_4GnnfRd"
      },
      "source": [
        "class Model2(tf.keras.Model):\r\n",
        "  \r\n",
        "  def __init__(self):\r\n",
        "    super(Model2,self).__init__()\r\n",
        "    #self.in_layer = tf.keras.Dense(30,activation='relu') # is this necessary?\r\n",
        "    self.cell = LSTM_Cell()\r\n",
        "    # nicht im Model sondern viell nur von au√üerhalb in die call Funktion geben\r\n",
        "    self.h_prev = np.zeros((batch_size,H_size))\r\n",
        "    self.C_prev = np.zeros((batch_size,H_size))\r\n",
        "    \r\n",
        "    self.out_layer = tf.keras.layers.Dense(units=3,activation='sigmoid')\r\n",
        "    \r\n",
        "  def reset_states(self):\r\n",
        "    self.h_prev = np.zeros((batch_size,H_size))\r\n",
        "    self.C_prev = np.zeros((batch_size,H_size))\r\n",
        "\r\n",
        "  # The call function takes the input over multipletimesteps and creates (and returns!)  \r\n",
        "  # the outputs over multiple timesteps.\r\n",
        "  def call(self,inputs):\r\n",
        "    #inputs = self.in_layer(inputs)\r\n",
        "    self.reset_states()\r\n",
        "    h_states = np.asarray([])#tf.TensorArray(tf.float32, size=0, dynamic_size=True, clear_after_read=False)\r\n",
        "    c_states = np.asarray([])#tf.TensorArray(tf.float32, size=0, dynamic_size=True, clear_after_read=False)\r\n",
        "    # feed sequential input into the cell (a batch of two digits and one sequence element at a time)\r\n",
        "    for t in tf.range(seq_length):\r\n",
        "      input = inputs[:,t,:]\r\n",
        "      # print(input.shape)\r\n",
        "      # compute and save hidden and cell states\r\n",
        "      self.h_prev,self.C_prev = self.cell(input,self.h_prev,self.C_prev)\r\n",
        "      # arrays to unroll the LSTM (outputs)\r\n",
        "      h_states = np.append(h_states,self.h_prev)#write(t,self.h_prev)\r\n",
        "      c_states = np.append(c_states,self.C_prev)#write(t,self.C_prev)\r\n",
        "    outputs = self.out_layer(self.h_prev)\r\n",
        "    # print(outputs)\r\n",
        "    return outputs"
      ],
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAD1EfDIh2iq"
      },
      "source": [
        "# Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g366yFrQanBm"
      },
      "source": [
        "def train_seq_generator():\r\n",
        "  while(True):\r\n",
        "    train_data = []\r\n",
        "    targets = []\r\n",
        "    for i in range(batch_size):\r\n",
        "      seq = []\r\n",
        "      d1 = np.random.randint(0,10)\r\n",
        "      d2 = np.random.randint(0,10)\r\n",
        "      counts = {x:0 for x in range(10)}\r\n",
        "      for x in range(seq_length):\r\n",
        "        ext = np.random.randint(0,10)\r\n",
        "        counts[ext]=counts[ext]+1\r\n",
        "        seq.append(ext)\r\n",
        "      label = [2] if counts[d1]<counts[d2] else [1] if counts[d1]>counts[d2] else [0] # d1,d2 equally often \r\n",
        "      seq = tf.constant(seq)\r\n",
        "      d1 = tf.one_hot(d1,depth=10)\r\n",
        "      d2 = tf.one_hot(d2,depth=10)\r\n",
        "      x_t_total = [[*d1,*d2,*tf.one_hot(elem,depth=10)] for elem in seq]\r\n",
        "      label = tf.squeeze([tf.reshape(tf.one_hot(label,depth=3,),(-1,))])\r\n",
        "      train_data.append(tf.constant(np.asarray(x_t_total)))\r\n",
        "      targets.append(label)\r\n",
        "    yield tf.constant(np.asarray(train_data)), tf.constant(np.asarray(targets))"
      ],
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKn8OULJj_gf"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSjhrpP4l8It"
      },
      "source": [
        "def train_step(model, input, target, loss_function, optimizer):\r\n",
        "    \r\n",
        "    # loss_object and optimizer_object are instances of respective tensorflow classes\r\n",
        "    with tf.GradientTape() as tape:\r\n",
        "        prediction = model(input)\r\n",
        "        loss = loss_function(target, prediction)\r\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\r\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n",
        "    \r\n",
        "    # accuracy using a 0.5 threshold\r\n",
        "    acc = np.sum(np.abs((target-prediction))<0.5)/(target.shape[0]*target.shape[1])\r\n",
        "\r\n",
        "    return np.mean(loss), acc\r\n"
      ],
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3i5TAntnECIO"
      },
      "source": [
        "# set initial parameters\r\n",
        "seq_length = 25 \r\n",
        "batch_size = 32\r\n",
        "num_batches = 20\r\n",
        "H_size = 2 # Size of the hidden state"
      ],
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6rf8JINlGBY"
      },
      "source": [
        "def train(model, steps=5, running_average_factor = 0.95):\n",
        "    \n",
        "    dataset = tf.data.Dataset.from_generator(\n",
        "      train_seq_generator,\n",
        "      output_signature=(\n",
        "          tf.TensorSpec(shape=(32,25,30), dtype=tf.float32, name='seq_data'),\n",
        "          tf.TensorSpec(shape=(32, 3), dtype=tf.float32, name='targets')))\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    cross_entropy_loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "    losses = []\n",
        "    acces = []\n",
        "\n",
        "    running_average_loss = 0\n",
        "    running_average_acc = 0\n",
        "    i=0\n",
        "    print_every=1\n",
        "\n",
        "    for i in range(steps):\n",
        "        # generating a new sample in each training step\n",
        "        [(seq,lbl)] = list(dataset.take(1))\n",
        "        loss, acc = train_step(model,seq, lbl, cross_entropy_loss, optimizer)\n",
        "        \n",
        "        # compute the running averages of training loss and accuracy\n",
        "        running_average_loss = running_average_factor * running_average_loss  + (1 - running_average_factor) * loss\n",
        "        running_average_acc = running_average_factor * running_average_acc  + (1 - running_average_factor) * acc\n",
        "        losses.append(running_average_loss)\n",
        "        acces.append(running_average_acc)\n",
        "        \n",
        "        if i%print_every==0:\n",
        "            print(f\"Training step {i}: average loss is {np.round(losses[-1],2)}, accuracy of {np.round(acces[-1], 2)} %\")\n",
        "        \n",
        "        if i == steps:\n",
        "            break\n",
        "        i+=1\n",
        "    return losses, acces"
      ],
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UzPfWxMrmYm",
        "outputId": "207442d5-7e46-4055-ec5b-8e03440fe6ae"
      },
      "source": [
        "# train the simple model\r\n",
        "model = Model2()\r\n",
        "losses, acces = train(model,steps=50)"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training step 0: average loss is 0.06, accuracy of 0.02 %\n",
            "Training step 1: average loss is 0.11, accuracy of 0.05 %\n",
            "Training step 2: average loss is 0.17, accuracy of 0.07 %\n",
            "Training step 3: average loss is 0.21, accuracy of 0.09 %\n",
            "Training step 4: average loss is 0.26, accuracy of 0.11 %\n",
            "Training step 5: average loss is 0.31, accuracy of 0.13 %\n",
            "Training step 6: average loss is 0.34, accuracy of 0.15 %\n",
            "Training step 7: average loss is 0.39, accuracy of 0.17 %\n",
            "Training step 8: average loss is 0.42, accuracy of 0.19 %\n",
            "Training step 9: average loss is 0.46, accuracy of 0.2 %\n",
            "Training step 10: average loss is 0.49, accuracy of 0.22 %\n",
            "Training step 11: average loss is 0.52, accuracy of 0.23 %\n",
            "Training step 12: average loss is 0.55, accuracy of 0.24 %\n",
            "Training step 13: average loss is 0.58, accuracy of 0.26 %\n",
            "Training step 14: average loss is 0.6, accuracy of 0.27 %\n",
            "Training step 15: average loss is 0.63, accuracy of 0.29 %\n",
            "Training step 16: average loss is 0.65, accuracy of 0.3 %\n",
            "Training step 17: average loss is 0.67, accuracy of 0.31 %\n",
            "Training step 18: average loss is 0.7, accuracy of 0.32 %\n",
            "Training step 19: average loss is 0.72, accuracy of 0.33 %\n",
            "Training step 20: average loss is 0.74, accuracy of 0.34 %\n",
            "Training step 21: average loss is 0.75, accuracy of 0.35 %\n",
            "Training step 22: average loss is 0.77, accuracy of 0.35 %\n",
            "Training step 23: average loss is 0.79, accuracy of 0.36 %\n",
            "Training step 24: average loss is 0.81, accuracy of 0.37 %\n",
            "Training step 25: average loss is 0.82, accuracy of 0.38 %\n",
            "Training step 26: average loss is 0.84, accuracy of 0.39 %\n",
            "Training step 27: average loss is 0.85, accuracy of 0.4 %\n",
            "Training step 28: average loss is 0.86, accuracy of 0.41 %\n",
            "Training step 29: average loss is 0.87, accuracy of 0.42 %\n",
            "Training step 30: average loss is 0.88, accuracy of 0.42 %\n",
            "Training step 31: average loss is 0.9, accuracy of 0.43 %\n",
            "Training step 32: average loss is 0.91, accuracy of 0.43 %\n",
            "Training step 33: average loss is 0.92, accuracy of 0.43 %\n",
            "Training step 34: average loss is 0.93, accuracy of 0.44 %\n",
            "Training step 35: average loss is 0.94, accuracy of 0.44 %\n",
            "Training step 36: average loss is 0.94, accuracy of 0.44 %\n",
            "Training step 37: average loss is 0.95, accuracy of 0.45 %\n",
            "Training step 38: average loss is 0.96, accuracy of 0.45 %\n",
            "Training step 39: average loss is 0.97, accuracy of 0.45 %\n",
            "Training step 40: average loss is 0.98, accuracy of 0.46 %\n",
            "Training step 41: average loss is 0.98, accuracy of 0.46 %\n",
            "Training step 42: average loss is 0.99, accuracy of 0.47 %\n",
            "Training step 43: average loss is 0.99, accuracy of 0.47 %\n",
            "Training step 44: average loss is 1.0, accuracy of 0.47 %\n",
            "Training step 45: average loss is 1.01, accuracy of 0.47 %\n",
            "Training step 46: average loss is 1.02, accuracy of 0.47 %\n",
            "Training step 47: average loss is 1.02, accuracy of 0.47 %\n",
            "Training step 48: average loss is 1.03, accuracy of 0.47 %\n",
            "Training step 49: average loss is 1.03, accuracy of 0.48 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}